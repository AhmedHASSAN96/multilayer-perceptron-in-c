# Implementation of Multi Layer Perceptron in C

Multi Layer perceptron (MLP) is an artificial neural network with one or more hidden layers between input and output layer. Refer the following figure:
![MLP Network with one input layer, two hidden layers and an output layer](/figures/mlp-network.png)
MLP's are fully connected (each hidden node is connected to each input node etc.). They use backpropagation as part of their learning phase. MLPs are widely used for pattern classification, recognition, prediction and approximation. Multi Layer Perceptron can solve problems which are not linearly separable.

#### References:

* https://www.coursera.org/lecture/machine-learning/backpropagation-algorithm-1z9WW
* https://gist.github.com/amirmasoudabdol/f1efda29760b97f16e0e
* https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c
* https://stackoverflow.com/questions/33058848/generate-a-random-double-between-1-and-1
* https://madalinabuzau.github.io/2016/11/29/gradient-descent-on-a-softmax-cross-entropy-cost-function.html
* http://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf
* https://jamesmccaffrey.wordpress.com/2017/06/23/two-ways-to-deal-with-the-derivative-of-the-relu-function/
* https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative
* https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
